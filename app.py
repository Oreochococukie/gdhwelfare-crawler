import streamlit as st
import subprocess
import sys
import pandas as pd
import time
from datetime import datetime, timedelta
import io

# ------------------------------------------------------
# [í•µì‹¬] Streamlit Cloudì—ì„œ Playwright ë¸Œë¼ìš°ì € ê°•ì œ ì„¤ì¹˜
# ------------------------------------------------------
@st.cache_resource
def install_playwright_browser():
    try:
        subprocess.run([sys.executable, "-m", "playwright", "install", "chromium"], check=True)
    except Exception as e:
        print(f"âŒ ì„¤ì¹˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

install_playwright_browser()

from playwright.sync_api import sync_playwright

# ------------------------------------------------------
# í¬ë¡¤ë§ ë¡œì§
# ------------------------------------------------------

def parse_date(date_str):
    try:
        return datetime.strptime(date_str.strip(), '%Y-%m-%d')
    except ValueError:
        try:
            return datetime.strptime(date_str.strip(), '%Y.%m.%d')
        except:
            return None

def scroll_to_bottom(page):
    """ìŠ¤í¬ë¡¤ì„ ë¶€ë“œëŸ½ê²Œ ë‚´ë ¤ì„œ ë°ì´í„° ë¡œë”© ìœ ë„"""
    try:
        # í•œ ë²ˆì— í™• ë‚´ë¦¬ì§€ ì•Šê³  ë‚˜ëˆ ì„œ ë‚´ë¦¼ (ë°ì´í„° ë¡œë”© íŠ¸ë¦¬ê±°)
        for _ in range(3):
            page.keyboard.press("End")
            time.sleep(0.5)
    except:
        pass

def scrape_with_period(start_date, end_date, progress_bar):
    data = []
    # URL ë’¤ì— ë¶ˆí•„ìš”í•œ íŒŒë¼ë¯¸í„° ì œê±°í•˜ê³  pageIndexë§Œ ë”± ë°”ê¿ˆ
    base_url = "https://www.gdhwelfare.or.kr/community/PhotoList.do?pageIndex={}"
    page_index = 1
    max_pages = 50 # ì•ˆì „ì„ ìœ„í•´ ìµœëŒ€ í˜ì´ì§€ ì œí•œ (í•„ìš”ì‹œ ëŠ˜ë¦¬ì„¸ìš”)
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True, args=['--no-sandbox', '--disable-dev-shm-usage'])
        # ëª¨ë°”ì¼ ë·°í¬íŠ¸ë¡œ ì„¤ì •í•˜ë©´ ë¦¬ìŠ¤íŠ¸ê°€ ë” ë‹¨ìˆœí•˜ê²Œ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ (ì„ íƒì‚¬í•­)
        context = browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        )
        page = context.new_page()
        
        try:
            while page_index <= max_pages:
                progress = page_index / max_pages
                progress_bar.progress(progress, text=f"ğŸ“„ {page_index}í˜ì´ì§€ ì½ëŠ” ì¤‘...")
                
                url = base_url.format(page_index)
                
                try:
                    # [í•µì‹¬ ìˆ˜ì •] networkidle -> domcontentloaded (ë¼ˆëŒ€ë§Œ ì˜¤ë©´ í†µê³¼)
                    # íƒ€ì„ì•„ì›ƒë„ 30ì´ˆë¡œ ì¤„ì—¬ì„œ ë¹¨ë¦¬ë¹¨ë¦¬ ë„˜ì–´ê°€ê²Œ í•¨
                    page.goto(url, wait_until='domcontentloaded', timeout=30000)
                except Exception as e:
                    st.error(f"{page_index}í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨ (ì¬ì‹œë„ í•„ìš”): {e}")
                    page_index += 1
                    continue

                # ë¦¬ìŠ¤íŠ¸ ìš”ì†Œê°€ ëœ° ë•Œê¹Œì§€ ì ê¹ ëŒ€ê¸° (ìµœëŒ€ 3ì´ˆ)
                try:
                    page.wait_for_selector(".list_in", state="attached", timeout=3000)
                except:
                    # ë¦¬ìŠ¤íŠ¸ê°€ ì•ˆ ëœ¨ë©´ ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ë¡œë”© ì‹¤íŒ¨ë¡œ ê°„ì£¼
                    st.write(f"âš ï¸ {page_index}í˜ì´ì§€ì— ê²Œì‹œë¬¼ì´ ì—†ê±°ë‚˜ ë¡œë”©ì´ ëŠ¦ìŠµë‹ˆë‹¤.")
                    break
                
                scroll_to_bottom(page)
                
                items = page.query_selector_all(".list_in")
                if not items:
                    st.write("ê²Œì‹œë¬¼ ì—†ìŒ, ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                page_has_valid = False
                current_page_collected = 0
                
                for item in items:
                    try:
                        title_elem = item.query_selector(".bold.ellipsis")
                        date_elem = item.query_selector(".photo_info > span:nth-child(2)")
                        # [ì¶”ê°€] ì‘ì„±ì ì—˜ë¦¬ë¨¼íŠ¸ ì„ íƒì
                        author_elem = item.query_selector(".photo_info > span:nth-child(1)")
                        
                        if title_elem and date_elem:
                            Title_ = title_elem.inner_text().strip()
                            Date_str = date_elem.inner_text().strip()
                            # [ì¶”ê°€] ì‘ì„±ì í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì—†ì„ ê²½ìš° 'ë¯¸ìƒ' ì²˜ë¦¬)
                            Author_ = author_elem.inner_text().strip() if author_elem else "ë¯¸ìƒ"

                            upload_date = parse_date(Date_str)
                            
                            if upload_date:
                                # ê¸°ê°„ ë‚´ ë°ì´í„°
                                if start_date <= upload_date <= end_date:
                                    # [ìˆ˜ì •] ë°ì´í„° ì €ì¥ ìˆœì„œ ë³€ê²½: ì œëª© -> ì‘ì„±ì -> ë‚ ì§œ
                                    data.append([Title_, Author_, Date_str])
                                    page_has_valid = True
                                    current_page_collected += 1
                                # ê¸°ê°„ ì§€ë‚œ ë°ì´í„° (ê³¼ê±° ë°ì´í„°) ë‚˜ì˜¤ë©´ ì¢…ë£Œ
                                elif upload_date < start_date:
                                    st.success(f"â¹ï¸ ì„¤ì •ëœ ê¸°ê°„({start_date.date()}) ì´ì „ ë°ì´í„° ë„ë‹¬. í¬ë¡¤ë§ ì¢…ë£Œ.")
                                    return data
                    except Exception:
                        continue
                
                # ë¡œê·¸ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                # st.write(f"âœ… {page_index}í˜ì´ì§€: {current_page_collected}ê±´ ìˆ˜ì§‘")

                # ì´ë²ˆ í˜ì´ì§€ì— ìœ íš¨í•œ ë°ì´í„°ê°€ í•˜ë‚˜ë„ ì—†ê³ , ì´ë¯¸ ê³¼ê±° ë‚ ì§œë„ ì•„ë‹ˆë¼ë©´? (ë¹ˆ í˜ì´ì§€ ë“±)
                if not page_has_valid and current_page_collected == 0:
                    # í˜¹ì‹œ ëª¨ë¥´ë‹ˆ ë‹¤ìŒ í˜ì´ì§€ë„ í•œ ë²ˆ ë” ê°€ë³´ê²Œ í•  ìˆ˜ë„ ìˆì§€ë§Œ, ë³´í†µì€ ì—¬ê¸°ì„œ ëëƒ„
                    pass 
                
                page_index += 1
                # ë„ˆë¬´ ë¹¨ë¦¬ ìš”ì²­í•˜ë©´ ì„œë²„ê°€ ì°¨ë‹¨í•  ìˆ˜ ìˆìœ¼ë‹ˆ 0.5ì´ˆ íœ´ì‹
                time.sleep(0.5)
            
            return data
        finally:
            browser.close()
    
    return data

# UI ì„¤ì •
st.set_page_config(page_title="GD ë³µì§€ í¬ë¡¤ëŸ¬", page_icon="ğŸ¢")

st.title("ğŸ¢ GD ë³µì§€ ì‚¬ì§„ ê²Œì‹œë¬¼ í¬ë¡¤ëŸ¬")
st.markdown("Playwright ì—”ì§„ (ê³ ì† ëª¨ë“œ) ê°€ë™ ì¤‘")

st.sidebar.header("ğŸ“… ì„¤ì •")
col1, col2 = st.sidebar.columns(2)
with col1:
    start_date = st.sidebar.date_input("ì‹œì‘ ë‚ ì§œ", value=datetime.now() - timedelta(days=30))
with col2:
    end_date = st.sidebar.date_input("ì¢…ë£Œ ë‚ ì§œ", value=datetime.now())

if st.button("ğŸš€ í¬ë¡¤ë§ ì‹œì‘", type="primary"):
    start_dt = datetime.combine(start_date, datetime.min.time())
    end_dt = datetime.combine(end_date, datetime.min.time())
    
    progress_bar = st.progress(0)
    data = scrape_with_period(start_dt, end_dt, progress_bar)
    progress_bar.progress(1.0, text="ì™„ë£Œ!")
    
    if data:
        # [ìˆ˜ì •] DataFrame ì»¬ëŸ¼ ìˆœì„œ ë³€ê²½: ì œëª©, ì‘ì„±ì, ë‚ ì§œ
        df = pd.DataFrame(data, columns=['ì œëª©', 'ì‘ì„±ì', 'ë‚ ì§œ'])
        st.success(f"ì´ {len(data)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ!")
        st.dataframe(df)
        
        output = io.BytesIO()
        df.to_excel(output, index=False, engine='openpyxl')
        output.seek(0)
        
        st.download_button(
            label="ğŸ“¥ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
            data=output.getvalue(),
            file_name=f"gd_welfare_{datetime.now().strftime('%Y%m%d')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
    else:
        st.warning("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ê°„ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
